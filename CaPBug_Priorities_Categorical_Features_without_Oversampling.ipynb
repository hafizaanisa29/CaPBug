{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Bug ID Product Component          Assignee    Status Classification  \\\n",
      "0  528905     JDT      Core    jdt-core-inbox  RESOLVED        Eclipse   \n",
      "1  530231     JDT      Core    jdt-core-inbox  RESOLVED        Eclipse   \n",
      "2  530758     JDT      Core          jarthana  VERIFIED        Eclipse   \n",
      "3  531990     JDT      Core    jdt-core-inbox  VERIFIED        Eclipse   \n",
      "4  532137     JDT      Core  register.eclipse  VERIFIED        Eclipse   \n",
      "\n",
      "  Priority            Opened  \\\n",
      "0       P1  12/18/2017 11:46   \n",
      "1       P1    1/24/2018 5:16   \n",
      "2       P1    2/5/2018 20:38   \n",
      "3       P1     3/5/2018 0:49   \n",
      "4       P1    3/7/2018 16:17   \n",
      "\n",
      "                                             Summary       Category  \n",
      "0  JDT UI Gerrit failing with \"invalid location f...  Configuration  \n",
      "1  Compilation failure in M20180123-0400 in jdt.c...  Configuration  \n",
      "2                    Build failure in I20180205-2000  Configuration  \n",
      "3                    Build failure in I20180304-2000  Configuration  \n",
      "4  Todays update produces compile error but javac...  Configuration  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('Bug Reports Dataset.csv')\n",
    "print (df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3    925\n",
      "P2    539\n",
      "P1    294\n",
      "P5    225\n",
      "P4    155\n",
      "Name: Priority, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "classes = df['Priority']\n",
    "print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    P1\n",
      "1    P1\n",
      "2    P1\n",
      "3    P1\n",
      "4    P1\n",
      "5    P1\n",
      "6    P1\n",
      "7    P1\n",
      "Name: Priority, dtype: object\n",
      "[0 0 0 0 0 0 0 0]\n",
      "0    P1\n",
      "1    P1\n",
      "2    P1\n",
      "3    P1\n",
      "4    P1\n",
      "5    P1\n",
      "6    P1\n",
      "7    P1\n",
      "8    P1\n",
      "9    P1\n",
      "Name: Priority, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform (classes)\n",
    "print(classes[:8])\n",
    "print(Y[:8])\n",
    "priority = df['Priority']\n",
    "print(priority[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical boolean mask\n",
    "categorical_feature_mask = df.dtypes == object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bug ID            False\n",
       "Product            True\n",
       "Component          True\n",
       "Assignee           True\n",
       "Status             True\n",
       "Classification     True\n",
       "Priority           True\n",
       "Opened             True\n",
       "Summary            True\n",
       "Category           True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_feature_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Product',\n",
       " 'Component',\n",
       " 'Assignee',\n",
       " 'Status',\n",
       " 'Classification',\n",
       " 'Priority',\n",
       " 'Opened',\n",
       " 'Summary',\n",
       " 'Category']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter categorical columns using mask and turn it into a list\n",
    "categorical_cols = df.columns[categorical_feature_mask].tolist()\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Product', 'Component', 'Assignee', 'Status', 'Classification', 'Priority', 'Opened', 'Category']\n"
     ]
    }
   ],
   "source": [
    "pri_col = []\n",
    "for i in categorical_cols:\n",
    "    if i != 'Summary':\n",
    "        pri_col.append(i)\n",
    "print(pri_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Component</th>\n",
       "      <th>Assignee</th>\n",
       "      <th>Status</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Opened</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>144</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>144</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>141</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>717</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>144</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>931</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>273</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>141</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1367</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>272</td>\n",
       "      <td>227</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1407</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>98</td>\n",
       "      <td>227</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>116</td>\n",
       "      <td>227</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1610</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Product  Component  Assignee  Status  Classification  Priority  Opened  \\\n",
       "0       44         41       144       4               4         0     475   \n",
       "1       44         41       144       4               4         0      82   \n",
       "2       44         41       141       6               4         0     717   \n",
       "3       44         41       144       6               4         0     931   \n",
       "4       44         41       273       6               4         0     948   \n",
       "5       44         41       141       6               4         0    1367   \n",
       "6       22         41         7       1               8         0     105   \n",
       "7       14        272       227       2               9         0    1407   \n",
       "8       14         98       227       5               9         0     149   \n",
       "9       14        116       227       2               9         0    1610   \n",
       "\n",
       "   Category  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "5         0  \n",
       "6         0  \n",
       "7         0  \n",
       "8         0  \n",
       "9         0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df[pri_col] = df[pri_col].apply(lambda col: le.fit_transform(col))\n",
    "df[pri_col].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    925\n",
       "1    539\n",
       "0    294\n",
       "4    225\n",
       "3    155\n",
       "Name: Priority, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Priority.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  = df[['Product','Classification','Component', 'Category', 'Assignee','Status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2138, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['Priority']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "text_classifier = RandomForestClassifier(n_estimators=100, random_state=0)  \n",
    "text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 38  11   8   0   2]\n",
      " [ 10  99  12   0   0]\n",
      " [  7  12 151   8   4]\n",
      " [  1   1  10  15   2]\n",
      " [  3   2   4   0  28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.64      0.64        59\n",
      "           1       0.79      0.82      0.80       121\n",
      "           2       0.82      0.83      0.82       182\n",
      "           3       0.65      0.52      0.58        29\n",
      "           4       0.78      0.76      0.77        37\n",
      "\n",
      "    accuracy                           0.77       428\n",
      "   macro avg       0.74      0.71      0.72       428\n",
      "weighted avg       0.77      0.77      0.77       428\n",
      "\n",
      "0.7733644859813084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,predictions))  \n",
    "print(classification_report(y_test,predictions))  \n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11  22  26   0   0]\n",
      " [  5  95  21   0   0]\n",
      " [  7  13 146   0  16]\n",
      " [  1   9  17   0   2]\n",
      " [  4   4  16   0  13]]\n",
      "0.5931191405623633\n",
      "0.6641604010025063\n",
      "0.6175559758083512\n",
      "0.6191588785046729\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.19      0.25        59\n",
      "           1       0.66      0.79      0.72       121\n",
      "           2       0.65      0.80      0.72       182\n",
      "           3       1.00      0.00      0.00        29\n",
      "           4       0.42      0.35      0.38        37\n",
      "\n",
      "    accuracy                           0.62       428\n",
      "   macro avg       0.62      0.43      0.41       428\n",
      "weighted avg       0.62      0.62      0.58       428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(metrics.precision_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)))\n",
    "print(metrics.recall_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)))\n",
    "print(metrics.f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test,y_pred,zero_division=1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "logreg = LogisticRegression(solver='liblinear', random_state=0)\n",
    "#fit the model\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_test = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1  25  33   0   0]\n",
      " [  0  81  40   0   0]\n",
      " [  5  21 155   0   1]\n",
      " [  0   9  20   0   0]\n",
      " [  0   5  28   0   4]]\n",
      "0.5931191405623633\n",
      "0.6641604010025063\n",
      "0.6175559758083512\n",
      "0.6191588785046729\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.02      0.03        59\n",
      "           1       0.57      0.67      0.62       121\n",
      "           2       0.56      0.85      0.68       182\n",
      "           3       1.00      0.00      0.00        29\n",
      "           4       0.80      0.11      0.19        37\n",
      "\n",
      "    accuracy                           0.56       428\n",
      "   macro avg       0.62      0.33      0.30       428\n",
      "weighted avg       0.56      0.56      0.48       428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred_test))  \n",
    "print(metrics.precision_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)))\n",
    "print(metrics.recall_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)))\n",
    "print(metrics.f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred)))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test,y_pred_test,zero_division=1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7336448598130841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "dtree_model = DecisionTreeClassifier(criterion = 'entropy').fit(X_train, y_train) \n",
    "dtree_predictions = dtree_model.predict(X_test) \n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, dtree_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 40  11   6   0   2]\n",
      " [ 16  94  10   1   0]\n",
      " [  9  13 141   7  12]\n",
      " [  1   1  11  14   2]\n",
      " [  3   2   6   1  25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.68      0.63        59\n",
      "           1       0.78      0.78      0.78       121\n",
      "           2       0.81      0.77      0.79       182\n",
      "           3       0.61      0.48      0.54        29\n",
      "           4       0.61      0.68      0.64        37\n",
      "\n",
      "    accuracy                           0.73       428\n",
      "   macro avg       0.68      0.68      0.67       428\n",
      "weighted avg       0.74      0.73      0.73       428\n",
      "\n",
      "0.7336448598130841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,dtree_predictions))  \n",
    "print(classification_report(y_test,dtree_predictions))  \n",
    "print(accuracy_score(y_test, dtree_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
